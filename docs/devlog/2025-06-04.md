# ðŸš€ Things to Look At If You Can't Scale â€” A Practical Guide to Bottlenecks

Everyone hits scaling ceilings. The trick is knowing where to look â€” and what to expect as you move up the RPS ladder.

In this post, Iâ€™ll share a field guide to bottlenecks, based on practical experience scaling systems to hundreds of thousands, and even millions, of requests per second (RPS).

Why this matters
Most systems donâ€™t scale linearly as RPS increases:

- What works at 100 RPS â†’ may hit first bottlenecks at 1K RPS.
- Fix that â†’ next bottleneck shows up at 10K.
- And so on â†’ like peeling an onion â†’ new challenges at each level.

## âœ… Checklist

### 100 RPS

- Single-threaded App?
- DB connection pool too small?
- Blocking IO?
- Latency within the application layer:
  - Frequent queries are not optimized.
  - Missing indexes.
- Underprovisioned database, service. 

Basic observability and code-profiling tools should help.

### 1K RPS

- CPU: Is app CPU-bound?
- GC pauses: Are you seeing stop-the-world pauses?
- DB: Is connection pool hitting limits?
- DB: Are writes or indexes becoming a problem?
- Load balancer: Is connection reuse working? (check keep-alive / timeouts)

### 10K RPS

- NIC bandwidth â†’ Are you saturating network card? (see node_network_transmit_bytes_total)
- Ingress controller: Can it handle this RPS?
- Kubernetes: Is kube-proxy tuning correct? (conntrack limits)
- DB: Disks â†’ WAL â†’ index bloat
- App: Are you mutex-locked internally?
- Are you seeing GC pressure? (heap tuning matters here)
- Are you using batching yet? (per-request writes wonâ€™t scale)

### âœ… 100K RPS

- vNIC limits! (virtual NICs â†’ ~2.5 Gbps â†’ flatline here)
- Load balancer queueing â†’ is LB your bottleneck?
- Is TLS offload needed? (TLS CPU still matters)
- Network packet loss? â†’ Are you seeing TCP retransmissions?
- DB: Have you sharded yet? Single Postgres wonâ€™t survive this alone.
- Queue system: Kafka? NATS?
- Consumer lag â†’ queues now critical bottleneck.
- Does Kubernetes control plane keep up with pod scheduling?

### âœ… Past 1M RPS

- NICs â†’ use physical 10 Gbps NICs, not vNICs.
- Load balancer â†’ dedicated HAProxy / Envoy nodes.
- Multi-level LB (global LB â†’ regional LB â†’ cluster ingress)
- Kafka tuning â†’ partition count / batch size
- Are your apps using batch APIs internally?
- Is DB using append-only / event-sourced model now?
- Cache hit ratio â†’ is cache now primary read path?
- DNS â†’ is latency from upstream resolver now a factor?
- CPU tuning â†’ are you pinning IRQs for NIC interrupts?

Tip: Beyond 1M RPS â†’ team size and process matter as much as tech. 
