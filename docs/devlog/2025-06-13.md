# Progress

1. Initial Setup
    - Use the kube-hetzner module to define a kube.tf with:
        1. One control plane node (cx21)
        2. One agent node (cpx11)
        3. One autoscaler node pool (cpx11, min=1, max=2)
        4. ARM snapshot: microos_arm_snapshot_id = "242591241"
   - SSH keys were initially marked as sensitive.

2. Debugging Deployment Issues
   - Terraform stalled at _Waiting for MicroOS to become available..._
     - [x] Verified that the snapshot worked by manually provisioning a server with microos_arm_snapshot_id. 
     - [x] Confirmed that SSH worked via CLI: ssh -i ~/.ssh/hetzner_ed25519 root@<IP>. \
     - [x] Resolved issue by making SSH key variables non-sensitve for a clean apply.
     - [x] Marked the variables sensitive on terraform cloud once the plan applied successfully.
     - [x] Tested by destroying and recreating the cluster with sensitive private key.

3. Successful Cluster Deployment
    ```
    38 resources created, including Hetzner Load Balancer, VMs, and autoscaler configuration.
    ```
4. Accessing the Cluster.
    - This required a terraform login (I am using terraform cloud) and terraform init.
    - Setting up terraform cloud as state backend.
        ```hcl
        terraform {
            backend "remote" {
                organization = "level-scale"
                workspaces {
                    name = "level-scale-dev-cluster"
                }
            }
        }
        ```
    - Extracted kubeconfig from Terraform output using:
        ```bash
        terraform output -raw kubeconfig > kubeconfig.yaml
        ```
    - Point `$KUBECONFIG` to the extracted:
       ```bash
       export KUBECONFIG=$PWD/kubeconfig.yaml
       ```

5. Verified cluster connection:
    ```
    kubectl get nodes
    NAME                         STATUS                     ROLES                       AGE   VERSION
    k3s-agent-fsn1-sm-yvy        Ready,SchedulingDisabled   <none>                      25m   v1.31.9+k3s1
    k3s-control-plane-fsn1-qiy   Ready                      control-plane,etcd,master   26m   v1.31.9+k3s1
    k3s-storage-fsn1-sm-qzb      Ready                      <none>                      25m   v1.31.9+k3s1
   ```

# Exposing cluster to the world

1. Create a nginx resource.
    ```
    kubectl create deployment hello --image=nginx
    kubectl expose deployment hello --port=80 --type=LoadBalancer
    ```
2. kubectl expose deployment hello --port=80 --target-port=80 --type=LoadBalancer
3. Initial issue: LoadBalancer returned ERR_EMPTY_RESPONSE 
   1. kubectl get endpoints hello showed no backend IPs 
       ```
       kubectl get endpoints hello
       NAME    ENDPOINTS   AGE
       hello   <none>      12s
       ```
   2. Root cause: no pod could be scheduled due to taints on all nodes and the agent node being cordoned by the node autoscheduler.
      ```
      ╰─⠠⠵ k get nodes
      NAME                         STATUS                     ROLES                       AGE   VERSION
      k3s-agent-fsn1-sm-yvy        Ready,SchedulingDisabled   <none>                      56m   v1.31.9+k3s1
      k3s-control-plane-fsn1-qiy   Ready                      control-plane,etcd,master   57m   v1.31.9+k3s1
      k3s-storage-fsn1-sm-qzb      Ready                      <none>                      56m   v1.31.9+k3s1
      
      ╰─⠠⠵  k describe pods hello*
      Events:
      Type     Reason            Age   From               Message
      ----     ------            ----  ----               -------
      Warning  FailedScheduling  33s   default-scheduler  0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }, 1 node(s) had untolerated taint {storage: postgres}, 1 node(s) were unschedulable. preemption: 0/3 nodes are available: 3 Preemption is not helpful for scheduling.
      ```
4. We can manually `uncordon` the node so that we can start deployments.
    ```
    kubectl uncordon k3s-agent-fsn1-sm-yvy
    
    ╰─⠠⠵ k get nodes
    NAME                         STATUS   ROLES                       AGE   VERSION
    k3s-agent-fsn1-sm-yvy        Ready    <none>                      56m   v1.31.9+k3s1
    k3s-control-plane-fsn1-qiy   Ready    control-plane,etcd,master   57m   v1.31.9+k3s1
    k3s-storage-fsn1-sm-qzb      Ready    <none>                      57m   v1.31.9+k3s1 

    ╰─⠠⠵ k get endpoints
    NAME         ENDPOINTS           AGE
    hello        10.42.0.14:80       3m32s
    ```
5. We also mention the target port. 
    ```
    kubectl expose deployment hello --port=80 --target-port=80 --type=LoadBalancer
    ```
6. Now if we use the public ip from 
    ```
    ╰─⠠⠵ k get svc hello
    NAME    TYPE           CLUSTER-IP    EXTERNAL-IP         PORT(S)        AGE
    hello   LoadBalancer   10.43.33.84   *.*.*.*,*:*:*:*::*  80:31062/TCP   16m
    ```
    Visiting http://$EXTERNAL-IP shows the nginx page!
